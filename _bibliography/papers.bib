---
---

@article{karkada2024lazy,
  title={The lazy (NTK) and rich (Î¼P) regimes: a gentle tutorial},
  author={Karkada, Dhruva},
  year={2024},
  preview={lazyrich-derivation.png},
  preview_title={Straightforward signal propagation arguments can directly constrain hyperparameters, yielding a one-dimensional 'richness' scale.},
  link={https://arxiv.org/abs/2404.19719}
}

@article{simon2023more,
  title={More is better in modern machine learning: when infinite overparameterization is optimal and overfitting is obligatory},
  author={Simon, James B and Karkada, Dhruva and Ghosh, Nikhil and Belkin, Mikhail},
  year={2023},
  preview={relu_features.png},
  preview_title={If regularization is chosen optimally, a random feature model always benefits from having more features (i.e., no U-curve nor double descent).},
  link={https://arxiv.org/abs/2311.14646}
}

@article{simon-eigenlearning-2022, 
  title={The eigenlearning framework: a conservation law perspective on KRR and wide NNs},
  author={Simon, James B. and Dickens, Maddie and Karkada, Dhruva and DeWeese, Michael R.},
  year={2022},
  preview={eigenlearningDB.png},
  preview_title={Our eigenlearning framework for kernel ridge regression explains the deep bootstrap phenomenon (divergence of train and test errors from an ideal online-learning curve).},
  link={https://arxiv.org/abs/2110.03922}
}