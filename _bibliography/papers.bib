---
---

@article{karkada2024lazy,
  title={The lazy (NTK) and rich (Î¼P) regimes: a gentle tutorial},
  author={Karkada, Dhruva},
  year={2024},
  preview={lazyrich-derivation.png},
  preview_title={Straightforward signal propagation arguments can directly constrain hyperparameters, yielding a one-dimensional 'richness' scale.},
  arxiv={2404.19719}
}

@article{simon2023more,
  title={More is better in modern machine learning: when infinite overparameterization is optimal and overfitting is obligatory},
  author={Simon, James B and Karkada, Dhruva and Ghosh, Nikhil and Belkin, Mikhail},
  year={2023},
  preview={relu_features.png},
  preview_title={If regularization is chosen optimally, a random feature model always benefits from having more features (i.e., no U-curve nor double descent).},
  arxiv={2311.14646},
  selected={true}
}

@article{simon-eigenlearning-2022, 
  title={The eigenlearning framework: a conservation law perspective on KRR and wide NNs},
  author={Simon, James B. and Dickens, Maddie and Karkada, Dhruva and DeWeese, Michael R.},
  year={2022},
  preview={eigenlearning.png},
  preview_title={We can now calculate the test error of wide, lazy neural networks. We use only the kernel eigensystem, which provides the necessary summary statistics regarding model-task alignment.},
  arxiv={2110.03922},
  selected={true}
}